#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¶é—´åºåˆ—è¶‹åŠ¿æ•°æ®ç®¡ç†å™¨
æ”¯æŒå†å²æ•°æ®ä¿å­˜å’Œæ—¶é—´åºåˆ—åˆ†æ
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from backend.scraper.crawlers.github_trending_html import GitHubTrendingHTMLCrawler
from backend.scraper.core.api_client import GitHubAPIClient

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('trending_time_series.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TimeSeriesTrendingManager:
    """æ—¶é—´åºåˆ—è¶‹åŠ¿æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self):
        self.api_client = GitHubAPIClient()
        # ä½¿ç”¨ GitHub Trending HTML çˆ¬è™«
        self.crawler = GitHubTrendingHTMLCrawler()
        
        # æ•°æ®å­˜å‚¨ç›®å½•
        self.base_dir = Path('public/trends')
        self.time_series_dir = self.base_dir / 'time_series'
        self.daily_dir = self.time_series_dir / 'daily'
        self.weekly_dir = self.time_series_dir / 'weekly'
        self.monthly_dir = self.time_series_dir / 'monthly'
        
        # åˆ›å»ºç›®å½•ç»“æ„
        for dir_path in [self.base_dir, self.time_series_dir, self.daily_dir, self.weekly_dir, self.monthly_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
    
    async def collect_and_store_trends(self, periods: List[str] = None) -> bool:
        """æ”¶é›†å¹¶å­˜å‚¨è¶‹åŠ¿æ•°æ®"""
        if periods is None:
            periods = ['daily', 'weekly', 'monthly']
        
        logger.info("[START] å¼€å§‹æ”¶é›†æ—¶é—´åºåˆ—è¶‹åŠ¿æ•°æ®...")

        try:
            # æ£€æŸ¥APIè¿æ¥
            if not self.api_client.test_connection():
                logger.error("GitHub API è¿æ¥å¤±è´¥")
                return False

            current_time = datetime.now()
            success_count = 0

            for period in periods:
                try:
                    logger.info(f"[INFO] æ”¶é›† {period} è¶‹åŠ¿æ•°æ®...")

                    # è·å–è¶‹åŠ¿æ•°æ®
                    data = await self._fetch_period_data(period)

                    if data:
                        # ä¿å­˜æ—¶é—´åºåˆ—æ•°æ®
                        await self._save_time_series_data(data, period, current_time)

                        # æ›´æ–°ä¸»è¶‹åŠ¿æ–‡ä»¶
                        await self._update_main_trends_file(data, period, current_time)

                        success_count += 1
                        logger.info(f"[SUCCESS] {period} æ•°æ®æ”¶é›†å®Œæˆ: {len(data)} ä¸ªé¡¹ç›®")
                    else:
                        logger.warning(f"[WARNING] {period} æ•°æ®æ”¶é›†å¤±è´¥")

                except Exception as e:
                    logger.error(f"[ERROR] {period} æ•°æ®æ”¶é›†å¼‚å¸¸: {e}")
                    continue
            
            # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
            await self._generate_summary_report(current_time)
            
            logger.info(f"ğŸ‰ è¶‹åŠ¿æ•°æ®æ”¶é›†å®Œæˆ: {success_count}/{len(periods)} ä¸ªæ—¶é—´æ®µæˆåŠŸ")
            return success_count > 0
            
        except Exception as e:
            logger.error(f"è¶‹åŠ¿æ•°æ®æ”¶é›†å¤±è´¥: {e}")
            return False
    
    async def _fetch_period_data(self, period: str) -> List[Dict[str, Any]]:
        """è·å–æŒ‡å®šæ—¶é—´æ®µçš„è¶‹åŠ¿æ•°æ®"""
        try:
            # æ ¹æ®æ—¶é—´æ®µè®¾ç½®ä¸åŒçš„å‚æ•°
            if period == 'daily':
                repos = self.crawler.fetch('daily')
            elif period == 'weekly':
                repos = self.crawler.fetch('weekly')
            else:  # monthly
                repos = self.crawler.fetch('monthly')
            
            # HTMLçˆ¬è™«å·²ç»è¿”å›äº†æ­£ç¡®æ ¼å¼çš„æ•°æ®ï¼Œæ— éœ€å†æ¬¡æ ¼å¼åŒ–
            return repos
            
        except Exception as e:
            logger.error(f"è·å– {period} æ•°æ®å¤±è´¥: {e}")
            return []
    
    def _format_repository_data(self, repo: Dict[str, Any], period: str) -> Dict[str, Any]:
        """æ ¼å¼åŒ–ä»“åº“æ•°æ®"""
        try:
            return {
                'id': repo.get('id'),
                'name': repo.get('name'),
                'owner': repo.get('owner', {}).get('login') if isinstance(repo.get('owner'), dict) else repo.get('owner'),
                'fullName': repo.get('full_name'),
                'description': repo.get('description', ''),
                'language': repo.get('language'),
                'stars': repo.get('stargazers_count', repo.get('stars', 0)),
                'forks': repo.get('forks_count', repo.get('forks', 0)),
                'todayStars': repo.get('today_stars', 0),
                'url': repo.get('html_url', repo.get('url')),
                'createdAt': repo.get('created_at'),
                'updatedAt': repo.get('updated_at'),
                'pushedAt': repo.get('pushed_at'),
                'topics': repo.get('topics', []),
                'trendPeriod': period,
                'trendDate': datetime.now().isoformat(),
                'collectedAt': datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–ä»“åº“æ•°æ®å¤±è´¥: {e}")
            return None
    
    async def _save_time_series_data(self, data: List[Dict[str, Any]], period: str, timestamp: datetime):
        """ä¿å­˜æ—¶é—´åºåˆ—æ•°æ®"""
        try:
            # ç”Ÿæˆæ–‡ä»¶åï¼šYYYY-MM-DD_HH-MM-SS.json
            filename = f"{timestamp.strftime('%Y-%m-%d_%H-%M-%S')}.json"
            
            # é€‰æ‹©ç›®å½•
            if period == 'daily':
                file_path = self.daily_dir / filename
            elif period == 'weekly':
                file_path = self.weekly_dir / filename
            else:  # monthly
                file_path = self.monthly_dir / filename
            
            # æ„å»ºæ•°æ®ç»“æ„
            time_series_data = {
                'timestamp': timestamp.isoformat(),
                'period': period,
                'count': len(data),
                'repositories': data,
                'metadata': {
                    'collectionTime': timestamp.isoformat(),
                    'apiVersion': '2024-01-24',
                    'dataVersion': '1.0'
                }
            }
            
            # ä¿å­˜æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(time_series_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"[SAVE] æ—¶é—´åºåˆ—æ•°æ®å·²ä¿å­˜: {file_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ—¶é—´åºåˆ—æ•°æ®å¤±è´¥: {e}")
    
    async def _update_main_trends_file(self, data: List[Dict[str, Any]], period: str, timestamp: datetime):
        """æ›´æ–°ä¸»è¶‹åŠ¿æ–‡ä»¶ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        try:
            trends_file = self.base_dir / 'trends.json'
            
            # è¯»å–ç°æœ‰æ•°æ®
            existing_data = {}
            if trends_file.exists():
                try:
                    with open(trends_file, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                except Exception as e:
                    logger.warning(f"è¯»å–ç°æœ‰trends.jsonå¤±è´¥: {e}")
            
            # æ›´æ–°å¯¹åº”æ—¶é—´æ®µçš„æ•°æ®
            existing_data[period] = data
            existing_data['lastUpdated'] = timestamp.isoformat()
            existing_data['metadata'] = existing_data.get('metadata', {})
            existing_data['metadata'].update({
                f'{period}Count': len(data),
                f'{period}LastUpdate': timestamp.isoformat()
            })
            
            # è®¡ç®—æ€»æ•°
            total_count = 0
            for p in ['daily', 'weekly', 'monthly']:
                if p in existing_data and isinstance(existing_data[p], list):
                    total_count += len(existing_data[p])
            existing_data['metadata']['totalCount'] = total_count
            
            # ä¿å­˜æ›´æ–°åçš„æ•°æ®
            with open(trends_file, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"[UPDATE] ä¸»è¶‹åŠ¿æ–‡ä»¶å·²æ›´æ–°: {trends_file}")
            
        except Exception as e:
            logger.error(f"æ›´æ–°ä¸»è¶‹åŠ¿æ–‡ä»¶å¤±è´¥: {e}")
    
    async def _generate_summary_report(self, timestamp: datetime):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        try:
            report_file = self.time_series_dir / 'collection_report.json'
            
            # ç»Ÿè®¡å„æ—¶é—´æ®µçš„æ•°æ®æ–‡ä»¶æ•°é‡
            daily_files = len(list(self.daily_dir.glob('*.json')))
            weekly_files = len(list(self.weekly_dir.glob('*.json')))
            monthly_files = len(list(self.monthly_dir.glob('*.json')))
            
            report = {
                'lastCollection': timestamp.isoformat(),
                'statistics': {
                    'dailySnapshots': daily_files,
                    'weeklySnapshots': weekly_files,
                    'monthlySnapshots': monthly_files,
                    'totalSnapshots': daily_files + weekly_files + monthly_files
                },
                'dataRetention': {
                    'dailyRetentionDays': 30,
                    'weeklyRetentionWeeks': 12,
                    'monthlyRetentionMonths': 12
                },
                'nextCollection': (timestamp + timedelta(hours=2)).isoformat()
            }
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"[REPORT] æ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šå¤±è´¥: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    manager = TimeSeriesTrendingManager()
    
    # æ”¶é›†è¶‹åŠ¿æ•°æ®
    success = await manager.collect_and_store_trends()
    
    if success:
        logger.info("[SUCCESS] æ—¶é—´åºåˆ—è¶‹åŠ¿æ•°æ®æ”¶é›†å®Œæˆ")
    else:
        logger.error("[ERROR] æ—¶é—´åºåˆ—è¶‹åŠ¿æ•°æ®æ”¶é›†å¤±è´¥")
    
    return success

if __name__ == "__main__":
    asyncio.run(main())
