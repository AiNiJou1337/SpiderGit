#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
GitHub è¶‹åŠ¿æ•°æ®ç®¡ç†å™¨
ç»Ÿä¸€ç®¡ç†æ¯æ—¥ã€æ¯å‘¨ã€æ¯æœˆçš„è¶‹åŠ¿æ•°æ®
"""

import os
import json
import asyncio
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
import sys
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# åŠ è½½.envæ–‡ä»¶
def load_env_file():
    """æ‰‹åŠ¨åŠ è½½.envæ–‡ä»¶"""
    env_path = project_root / '.env'
    if env_path.exists():
        try:
            with open(env_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        key = key.strip()
                        value = value.strip().strip('"').strip("'")
                        if key and value:
                            os.environ[key] = value
            print(f"[SUCCESS] å·²åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_path}")
            return True
        except Exception as e:
            print(f"[WARNING] åŠ è½½.envæ–‡ä»¶å¤±è´¥: {e}")
            return False
    else:
        print(f"[WARNING] æœªæ‰¾åˆ°.envæ–‡ä»¶: {env_path}")
        return False

# å°è¯•åŠ è½½ç¯å¢ƒå˜é‡
load_env_file()

from backend.scraper.core.api_client import GitHubAPIClient
from backend.scraper.crawlers.github_trending_html import GitHubTrendingHTMLCrawler

logger = logging.getLogger(__name__)

class TrendingDataManager:
    """è¶‹åŠ¿æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self):
        self.api_client = GitHubAPIClient()
        # ä½¿ç”¨ GitHub Trending HTML çˆ¬è™«ä½œä¸ºæ•°æ®æº
        self.crawler = GitHubTrendingHTMLCrawler()
        self.data_dir = Path('public/trends/data')
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
    async def fetch_and_save_all_trends(self):
        """è·å–å¹¶ä¿å­˜æ‰€æœ‰æ—¶é—´æ®µçš„è¶‹åŠ¿æ•°æ®"""
        logger.info("å¼€å§‹è·å–æ‰€æœ‰è¶‹åŠ¿æ•°æ®...")
        
        try:
            # æ£€æŸ¥APIè¿æ¥
            if not self.api_client.test_connection():
                logger.error("GitHub API è¿æ¥å¤±è´¥")
                return False
            
            # è·å–ä¸‰ä¸ªæ—¶é—´æ®µçš„æ•°æ®
            daily_data = await self._fetch_period_data('daily')
            weekly_data = await self._fetch_period_data('weekly') 
            monthly_data = await self._fetch_period_data('monthly')
            
            # æ„å»ºå®Œæ•´çš„è¶‹åŠ¿æ•°æ®
            trends_data = {
                'daily': daily_data,
                'weekly': weekly_data,
                'monthly': monthly_data,
                'lastUpdated': datetime.now().isoformat(),
                'metadata': {
                    'dailyCount': len(daily_data),
                    'weeklyCount': len(weekly_data),
                    'monthlyCount': len(monthly_data),
                    'totalCount': len(daily_data) + len(weekly_data) + len(monthly_data)
                }
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            await self._save_trends_data(trends_data)
            
            logger.info(f"[SUCCESS] è¶‹åŠ¿æ•°æ®è·å–å®Œæˆ: æ—¥({len(daily_data)}) å‘¨({len(weekly_data)}) æœˆ({len(monthly_data)})")
            return True
            
        except Exception as e:
            logger.error(f"è·å–è¶‹åŠ¿æ•°æ®å¤±è´¥: {e}")
            return False
    
    async def _fetch_period_data(self, period: str) -> List[Dict[str, Any]]:
        """è·å–æŒ‡å®šæ—¶é—´æ®µçš„è¶‹åŠ¿æ•°æ®"""
        logger.info(f"è·å– {period} è¶‹åŠ¿æ•°æ®...")

        try:
            all_repos = []

            # 1. è·å–å…¨è¯­è¨€è¶‹åŠ¿æ•°æ®
            logger.info(f"è·å– {period} å…¨è¯­è¨€è¶‹åŠ¿æ•°æ®...")
            general_repos = self.crawler.fetch(period)
            all_repos.extend(general_repos)
            logger.info(f"å…¨è¯­è¨€æ•°æ®: {len(general_repos)} ä¸ªé¡¹ç›®")

            # 2. è·å–çƒ­é—¨è¯­è¨€çš„è¶‹åŠ¿æ•°æ®
            popular_languages = [
                'javascript', 'python', 'java', 'typescript', 'c++', 'c#', 'php',
                'c', 'shell', 'go', 'rust', 'kotlin', 'swift', 'dart', 'ruby',
                'scala', 'r', 'matlab', 'perl', 'lua', 'haskell', 'clojure',
                'vue', 'html', 'css', 'scss', 'less'
            ]

            for lang in popular_languages:
                try:
                    logger.info(f"è·å– {period} {lang} è¶‹åŠ¿æ•°æ®...")
                    lang_repos = self.crawler.fetch(period, language=lang)
                    if lang_repos:
                        all_repos.extend(lang_repos)
                        logger.info(f"{lang} æ•°æ®: {len(lang_repos)} ä¸ªé¡¹ç›®")

                    # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™åˆ¶
                    await asyncio.sleep(1)

                except Exception as e:
                    logger.warning(f"è·å– {lang} æ•°æ®å¤±è´¥: {e}")
                    continue

            # 3. å»é‡ï¼ˆåŸºäºfull_nameï¼‰
            seen = set()
            unique_repos = []
            for repo in all_repos:
                full_name = repo.get('full_name')
                if full_name and full_name not in seen:
                    seen.add(full_name)
                    unique_repos.append(repo)

            logger.info(f"{period} æ•°æ®è·å–å®Œæˆ: {len(unique_repos)} ä¸ªå»é‡åçš„é¡¹ç›®")
            return unique_repos

        except Exception as e:
            logger.error(f"è·å– {period} æ•°æ®å¤±è´¥: {e}")
            return []
    
    async def _get_recent_trending(self, days: int, min_stars: int, max_results: int) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘æŒ‡å®šå¤©æ•°çš„è¶‹åŠ¿é¡¹ç›®"""
        since_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        
        # æ„å»ºå¤šä¸ªæŸ¥è¯¢ä»¥è·å–æ›´å¤šæ ·åŒ–çš„æ•°æ®
        queries = [
            f'created:>{since_date} stars:>{min_stars}',
            f'pushed:>{since_date} stars:>{min_stars//2}',
            f'created:>{since_date} language:JavaScript stars:>5',
            f'created:>{since_date} language:Python stars:>5',
            f'created:>{since_date} language:TypeScript stars:>5',
        ]
        
        all_repos = []
        seen_repos = set()
        
        for query in queries:
            try:
                # è·å–å¤šé¡µæ•°æ®
                for page in range(1, 3):  # æœ€å¤š2é¡µ
                    search_result = self.api_client.search_repositories(
                        query=query,
                        sort='stars',
                        order='desc',
                        per_page=50,
                        page=page
                    )
                    
                    if not search_result or 'items' not in search_result:
                        break
                    
                    for repo in search_result['items']:
                        repo_key = repo.get('full_name')
                        if repo_key and repo_key not in seen_repos:
                            seen_repos.add(repo_key)
                            all_repos.append(repo)
                            
                            if len(all_repos) >= max_results:
                                break
                    
                    if len(all_repos) >= max_results:
                        break
                    
                    await asyncio.sleep(1)  # é¿å…é€Ÿç‡é™åˆ¶
                
                if len(all_repos) >= max_results:
                    break
                    
                await asyncio.sleep(2)  # æŸ¥è¯¢é—´éš”
                
            except Exception as e:
                logger.warning(f"æŸ¥è¯¢å¤±è´¥ '{query}': {e}")
                continue
        
        # æŒ‰æ˜Ÿæ•°æ’åº
        all_repos.sort(key=lambda x: x.get('stargazers_count', 0), reverse=True)
        return all_repos[:max_results]
    
    def _format_repository_data(self, repo: Dict[str, Any], period: str) -> Optional[Dict[str, Any]]:
        """æ ¼å¼åŒ–ä»“åº“æ•°æ®"""
        try:
            # ä» HTML æºè¯»å–ä»Šæ—¥å¢é•¿
            stars = repo.get('stargazers_count', repo.get('stars', 0))
            today = repo.get('today_stars', 0)

            return {
                'id': repo.get('id'),
                'name': repo.get('name'),
                'owner': repo.get('owner', {}).get('login') if isinstance(repo.get('owner'), dict) else str(repo.get('owner', '')),
                'fullName': repo.get('full_name'),
                'description': repo.get('description'),
                'language': repo.get('language'),
                'stars': stars,
                'forks': repo.get('forks_count', repo.get('forks', 0)),
                'todayStars': today,
                'url': repo.get('html_url'),
                'createdAt': repo.get('created_at'),
                'updatedAt': repo.get('updated_at'),
                'pushedAt': repo.get('pushed_at'),
                'topics': repo.get('topics', []),
                'trendPeriod': period,
                'scrapedAt': datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–ä»“åº“æ•°æ®å¤±è´¥: {e}")
            return None
    
    async def _save_trends_data(self, data: Dict[str, Any]):
        """ä¿å­˜è¶‹åŠ¿æ•°æ®"""
        try:
            # ä¿å­˜ä¸»è¦çš„trends.jsonæ–‡ä»¶
            trends_file = self.data_dir / 'trends.json'
            with open(trends_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            logger.info(f"è¶‹åŠ¿æ•°æ®å·²ä¿å­˜åˆ°: {trends_file}")

            # åŒæ—¶ä¿å­˜å¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_file = self.data_dir / f'trends_backup_{timestamp}.json'
            with open(backup_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            logger.info(f"å¤‡ä»½æ–‡ä»¶å·²ä¿å­˜åˆ°: {backup_file}")

        except Exception as e:
            logger.error(f"ä¿å­˜è¶‹åŠ¿æ•°æ®å¤±è´¥: {e}")
    
    def get_rate_limit_status(self) -> Optional[Dict[str, Any]]:
        """è·å–APIé€Ÿç‡é™åˆ¶çŠ¶æ€"""
        try:
            return self.api_client.get_rate_limit()
        except Exception as e:
            logger.error(f"è·å–é€Ÿç‡é™åˆ¶çŠ¶æ€å¤±è´¥: {e}")
            return None

async def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•å’Œæ‰‹åŠ¨æ‰§è¡Œ"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger.info("[START] å¯åŠ¨è¶‹åŠ¿æ•°æ®ç®¡ç†å™¨")

    # ä½¿ç”¨TokenManageræ£€æŸ¥ç¯å¢ƒå˜é‡
    from backend.scraper.core.token_manager import GitHubTokenManager

    try:
        token_manager = GitHubTokenManager()
        if not token_manager.tokens:
            logger.error("[ERROR] æœªæ‰¾åˆ°GitHub Tokenç¯å¢ƒå˜é‡")
            logger.error("è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®GitHub Tokenå˜é‡")
            return

        logger.info(f"[SUCCESS] åŠ è½½äº† {len(token_manager.tokens)} ä¸ªToken")

    except Exception as e:
        logger.error(f"[ERROR] Tokenç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    manager = TrendingDataManager()
    
    # æ£€æŸ¥APIçŠ¶æ€
    rate_limit = manager.get_rate_limit_status()
    if rate_limit:
        remaining = rate_limit.get('remaining', 0)
        limit = rate_limit.get('limit', 0)
        logger.info(f"[INFO] APIçŠ¶æ€: {remaining}/{limit} å‰©ä½™")

        if remaining < 100:
            logger.warning("[WARNING] APIè°ƒç”¨æ¬¡æ•°ä¸è¶³ï¼Œå»ºè®®ç¨åå†è¯•")
    
    # æ‰§è¡Œæ•°æ®è·å–
    success = await manager.fetch_and_save_all_trends()
    
    if success:
        logger.info("ğŸ‰ è¶‹åŠ¿æ•°æ®æ›´æ–°å®Œæˆï¼")
    else:
        logger.error("âŒ è¶‹åŠ¿æ•°æ®æ›´æ–°å¤±è´¥")

if __name__ == "__main__":
    asyncio.run(main())
